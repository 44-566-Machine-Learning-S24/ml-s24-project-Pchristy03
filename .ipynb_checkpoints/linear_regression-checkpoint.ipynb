{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dcc9c57-81b8-4d9f-a3ae-9f5ffbd974f1",
   "metadata": {},
   "source": [
    "# Part 1 - Initial Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c48d2aa4-e946-4179-9562-e0340365da1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulling in the dataset and looking at its values\n",
    "import pandas as pd\n",
    "dataframe = pd.read_csv(\"Online Retail.csv\")\n",
    "\n",
    "\n",
    "# Cleaning the data so that we don't have large outliers that skew data\n",
    "# limit some of the data down and clean it so that we can see values on the graph properly\n",
    "dataframe = dataframe[dataframe['UnitPrice'] >= 0]\n",
    "dataframe = dataframe[dataframe['UnitPrice'] <= 100]\n",
    "dataframe = dataframe[dataframe['Quantity'] >= 0]\n",
    "dataframe = dataframe[dataframe['Quantity'] <= 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9a5ba6d-52de-4aca-9195-64d7908afa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep cleaning\n",
    "\n",
    "# create new columns containing individual pieces from the InvoiceDate\n",
    "dataframe[\"InvoiceDate\"] = pd.to_datetime(dataframe[\"InvoiceDate\"], format='%Y-%m-%d %H:%M:%S')\n",
    "dataframe['trans_month'] = dataframe['InvoiceDate'].dt.month\n",
    "dataframe['trans_week'] = dataframe['InvoiceDate'].dt.isocalendar().week\n",
    "dataframe[\"trans_day\"] = dataframe['InvoiceDate'].dt.dayofyear\n",
    "\n",
    "#Creating a total_cost feature for each row and limiting the working set to rows that\n",
    "# contain total cost values less than 2000. This limit gets rid of outliers that greatly increase the graph size\n",
    "dataframe['total_cost'] = dataframe['Quantity'] * dataframe['UnitPrice']\n",
    "dataframe = dataframe[dataframe['total_cost'] <= 2000]\n",
    "\n",
    "# calculate weekly total spending\n",
    "weekly_total = []\n",
    "for i in range(1, 53):\n",
    "    weekly_total.append(dataframe[dataframe[\"trans_week\"] == i][\"UnitPrice\"].sum())\n",
    "\n",
    "# add the average weekly total to each row\n",
    "mapping_dict = dict(zip(range(1, 53), weekly_total))\n",
    "dataframe[\"weekly_total\"] = dataframe['trans_week'].map(mapping_dict)\n",
    "\n",
    "# calculate weekly spending average\n",
    "average_weekly_cost = []\n",
    "for i in range(1, 53):\n",
    "    average_weekly_cost.append(dataframe[dataframe[\"trans_week\"] == i][\"total_cost\"].mean())\n",
    "\n",
    "# add the average weekly total to each row\n",
    "mapping_dict = dict(zip(range(1, 53), average_weekly_cost))\n",
    "dataframe[\"average_weekly_total\"] = dataframe['trans_week'].map(mapping_dict)\n",
    "\n",
    "# calculate daily spending average\n",
    "average_daily_cost = []\n",
    "for i in range(1, 366):\n",
    "    average_daily_cost.append(dataframe[dataframe[\"trans_day\"] == i][\"UnitPrice\"].mean())\n",
    "\n",
    "# add the average daily total to each row\n",
    "mapping_dict = dict(zip(range(1, 366), average_daily_cost))\n",
    "dataframe[\"average_daily_total\"] = dataframe['trans_day'].map(mapping_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7be1c52f-f2a3-435b-8feb-2ff8e7ae436a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# create both sets with a test size of 20 percent\n",
    "train_set, test_set = train_test_split(dataframe, test_size=0.2, random_state=46)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81db27b9-9e3a-4ed0-a356-6db57b1300e5",
   "metadata": {},
   "source": [
    "# Part 2 - Picking Feature/s for X and the target feature y\n",
    "\n",
    "For the features for X I am choosing  \"trans_week\", \"weekly_total\", \"average_daily_total\" as they should give a good insight into how much a person may spend on a weekly basis. By looking at the transaction week, the weekly total for these transactions, and the average spending over a daily time I think that we could get some use predictive power out of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b197bcb9-b95c-4082-a0c5-7ae1bdcf3e41",
   "metadata": {},
   "source": [
    "# Part 3 - Doing Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ea49626-f21f-4076-a86f-6dfcece41d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score: 0.0736282081512738\n",
      "RMSE: 2.1318226059088436\n"
     ]
    }
   ],
   "source": [
    "# Doing Linear Regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = LinearRegression()\n",
    "\n",
    "X = train_set[[\"trans_week\", \"weekly_total\", \"average_daily_total\"]]\n",
    "y = train_set[\"average_weekly_total\"]\n",
    "\n",
    "linreg.fit(X, y)\n",
    "\n",
    "Ypred = linreg.predict(X)\n",
    "print(\"R2 Score:\", linreg.score(X, y))\n",
    "print(\"RMSE:\", mean_squared_error(y, Ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac80d230-fbf8-44fd-836d-ab6d7d1f69c3",
   "metadata": {},
   "source": [
    "# Part 4 - Result Comments\n",
    "\n",
    "### R2 result\n",
    "```R2 Score: 0.0736282081512738```\n",
    "\n",
    "When taking a look at the R2 score it is defintely not great. We should be shooting for something that is closer to 1. While I think there is definitely a relation ship here I don't think that it will be visible until some transformations are done to the data. When taking a look at the the values in weekly total they are far larger than the values in average_daily_total and average_weekly_total. I think this causes a large spread in the data and then ends up causing this very low R2 score. Moving forward I think this data should be scaled properly and I ultimately don't think the relationship here is linear. \n",
    "\n",
    "\n",
    "### RMSE Result\n",
    "```RMSE: 2.1318226059088436```\n",
    "\n",
    "The RMSE error of 2.13 is not great either. Its saying that on average the results of the model will deviate by 2.13 units during prediction. If we were to be predicting super large values this would be acceptable, but since we are ultimately trying to predict average weekly spending results we would want to minimize this value as much as possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907edbfa-f54a-427d-a15b-74350d50de44",
   "metadata": {},
   "source": [
    "# Part 5 - Trying to Improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bfafa0a-7757-4526-8426-69ef6b5387b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new features that may help increase the linear regression of the model\n",
    "\n",
    "# grab the number of transactions per week and day\n",
    "weekly_transactions = dataframe.groupby('trans_week').size()\n",
    "daily_transactions = dataframe.groupby('trans_day').size()\n",
    "\n",
    "#create weekly and daily transaction rows. Assign the values to the correct day or week value\n",
    "mapping_dict = dict(zip(range(1, 53), weekly_transactions))\n",
    "dataframe[\"weekly_transactions\"] = dataframe['trans_week'].map(mapping_dict)\n",
    "\n",
    "mapping_dict = dict(zip(range(1, 366), daily_transactions))\n",
    "dataframe[\"daily_transactions\"] = dataframe['trans_day'].map(mapping_dict)\n",
    "\n",
    "# so we can perform linear regression remove the rows that contain NaN values\n",
    "dataframe.replace('NaN', None, inplace=True)\n",
    "dataframe = dataframe.dropna(subset=['daily_transactions'])\n",
    "\n",
    "#split again with newly added features\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# create both sets with a test size of 20 percent\n",
    "second_train_set, second_test_set = train_test_split(dataframe, test_size=0.2, random_state=46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2236c29-953b-45c3-8b3b-02d0566181b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score: 0.04650859915571137\n",
      "RMSE: 2.6838720458266185\n"
     ]
    }
   ],
   "source": [
    "# Running the regression again on the new features\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = LinearRegression()\n",
    "\n",
    "X = second_train_set[[\"daily_transactions\", \"weekly_transactions\", \"average_daily_total\"]]\n",
    "y = second_train_set[\"average_weekly_total\"]\n",
    "\n",
    "linreg.fit(X, y)\n",
    "Ypred = linreg.predict(X)\n",
    "\n",
    "print(\"R2 Score:\", linreg.score(X, y))\n",
    "print(\"RMSE:\", mean_squared_error(y, Ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4ae222-1c09-4c2b-9fc4-f8f7f0c82a61",
   "metadata": {},
   "source": [
    "## Improvement Attempt Comments\n",
    "\n",
    "Original Scores\n",
    "```\n",
    "R2 Score: 0.0736282081512738\n",
    "RMSE: 2.1318226059088436\n",
    "```\n",
    "\n",
    "New Scores\n",
    "```\n",
    "R2 Score: 0.04650859915571137\n",
    "RMSE: 2.6838720458266185\n",
    "```\n",
    "\n",
    "Adding the weekly and daily transactions to the model did not improve the overall usefulness of the model. It actually dropped the R2 value even lower and added nearly .5 to the RMSE. After further analysis of the newly created features it can be seen that daily_transactions and week_transactions values are much much larger than that of the average_daily_total. I think this is causing a big spread in the data and ultimately base scores on this model. Overall, further analysis of the data will be needed, and potentially a different model other than a line\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51392265-0d9c-4abe-99a7-f00c41528d0b",
   "metadata": {},
   "source": [
    "# Part 6 - Final Test Set Evaluation\n",
    "\n",
    "Since I got marginally better results with the first set of data that I used I am going to stick with that one when going through the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb0d5c64-5fb4-4df6-89ac-a812ee8ea06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16.14587145 16.02963029 16.10768012 ... 15.9319921  16.20004547\n",
      " 16.29532055]\n",
      "R2: 0.07267753822610223\n",
      "rms: 2.129693450703618\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_eval = test_set[[\"trans_week\", \"weekly_total\", \"average_daily_total\"]]\n",
    "y_true = test_set[\"average_weekly_total\"]\n",
    "y_pred = linreg.predict(X_eval)\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "r2 = linreg.score(X_eval, y_true)\n",
    "rms_error = mean_squared_error(y_true, y_pred)\n",
    "print(\"R2:\", r2)\n",
    "print(\"rms:\", rms_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325598a8-9698-4460-8539-81169d43ae51",
   "metadata": {},
   "source": [
    "# Final Evaluation Discussion\n",
    "\n",
    "After running the model on the **test_set** I ultimately got the same results. The R2 value changed by ~0.001 and the RMSE changed by less than 0.01. For the inputs I feel that they all provide good insight into the value being predicted, but I don't think its a linear correlation. Its easy to see in my initial exploration that there is a non linear relationship between the weeks of the year and the amounts of spending, and total transactions. I think that I could capture that here as well by properly scaling the values and fitting a different function to the data thats not linear. \n",
    "\n",
    "Average_daily_total and weekly_total, in my understanding of this data, provide an important point to how much spending could occur depending on the week and day of the year. For the value being predicted its exactly as the name implies. Its a set of data that contains average week spending amounts. This value should have some relation (seems to be non linear) to the values being used to predict it. When looking at the values being predicted they do seem to fall in range of what the actual averages are (around 13 - 14) which explains why the RMSE is 2.12. From my exploration on this notebook and the previous one I believe the relationship is relatively constant until we reach points towards the end of the year. Here it grows exponentially and then quickly falls back down. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21753df6-f2fa-450e-84ab-4bc7847e78fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
